{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import data_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.set_cmap('RdYlBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"dataset/train.csv\").fillna('unknown')\n",
    "test = data_loader.load_test_data('dataset/test.csv','dataset/test_labels.csv').fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl = {\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" frown \",\n",
    "    \":(\": \" frown \",\n",
    "    \":s\": \" frown \",\n",
    "    \":-s\": \" frown \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = []\n",
    "new_test_data = []\n",
    "\n",
    "list_train = train['comment_text'].tolist()\n",
    "list_test = test['comment_text'].tolist()\n",
    "\n",
    "for i in list_train:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in repl.keys():\n",
    "            j = repl[j]\n",
    "        xx = xx + j + \" \"\n",
    "    new_train_data.append(xx)\n",
    "\n",
    "for i in list_test:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in repl.keys():\n",
    "            j = repl[j]\n",
    "        xx = xx + j + \" \"\n",
    "    new_test_data.append(xx)\n",
    "\n",
    "train[\"clean_comment_text\"] = new_train_data\n",
    "test[\"clean_comment_text\"] = new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z ?!]+')\n",
    "train_text = train[\"clean_comment_text\"].tolist()\n",
    "test_text = test[\"clean_comment_text\"].tolist()\n",
    "for i,c in enumerate(train_text):\n",
    "    train_text[i] = pattern.sub('',train_text[i].lower())\n",
    "for i,c in enumerate(test_text):\n",
    "    test_text[i] = pattern.sub('',test_text[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train_text\n",
    "test[\"comment_text\"] = test_text\n",
    "del train_text, test_text\n",
    "train.drop(['clean_comment_text'], inplace = True, axis = 1)\n",
    "test.drop(['clean_comment_text'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([train['comment_text'],test['comment_text']])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(ngram_range =(1,3),\n",
    "                             tokenizer=tokenize,\n",
    "                             min_df=3, max_df=0.9,\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words = 'english',\n",
    "                             analyzer = 'word',\n",
    "                             use_idf=1,\n",
    "                             smooth_idf=1,\n",
    "                             sublinear_tf=1 )\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(ngram_range =(1,4),\n",
    "                                 min_df=3, max_df=0.9,\n",
    "                                 strip_accents='unicode',\n",
    "                                 analyzer = 'char',\n",
    "                                 stop_words = 'english',\n",
    "                                 use_idf=1,\n",
    "                                 smooth_idf=1,\n",
    "                                 sublinear_tf=1,\n",
    "                                 max_features=50000)\n",
    "\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer)\n",
    "\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "train_matrix =vectorizer.transform(train['comment_text'])\n",
    "test_matrix = vectorizer.transform(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class_names = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = []\n",
    "def cross_validation(model,y_train):\n",
    "    score = cross_val_score(model,train_matrix,y_train,scoring='accuracy',cv=5)\n",
    "    val_score.append(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = train_matrix[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=3, dual=True)\n",
    "    x_nb = train_matrix.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((len(test), len(class_names)))\n",
    "\n",
    "for i, j in enumerate(class_names):\n",
    "    m,r = get_mdl(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_matrix.multiply(r))[:,1]\n",
    "    \n",
    "np.save(\"nblr-svm_test_predict.npy\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission\n",
    "subm = pd.read_csv('dataset/sample_submission.csv')\n",
    "predictions = np.load('nblr-svm_test_predict.npy')\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(predictions, columns = class_names)], axis=1)\n",
    "submission.to_csv('nblr-svm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
